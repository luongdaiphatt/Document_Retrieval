{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b83c0933-054d-4d41-b48f-30711fb9e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time as t\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# disable warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "425b2120-18af-441a-9eaf-36b564f6e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtc_url = \"https://vtcnews.vn/\"\n",
    "sub_page_vtc=['thoi-su-28', 'kinh-te-29', 'the-thao-34', 'the-gioi-30',\n",
    "              'giao-duc-31', 'phap-luat-32', 'giai-tri-33', 'suc-khoe-35', \n",
    "              'khoa-hoc-cong-nghe-82', 'oto-xe-may-37', \n",
    "              'phong-su-kham-pha-36', 'song-ket-noi-123']\n",
    "vtc_news = [vtc_url + i for i in sub_page_vtc]\n",
    "\n",
    "lao_dong_url = \"https://laodong.vn/\"\n",
    "sub_page_ld = [\"xa-hoi\", \"moi-truong\", \"giao-thong\", \"kinh-doanh\", \n",
    "         \"thoi-su\", \"the-gioi\", \"phap-luat\", \"tu-van-phap-luat\", \n",
    "         \"an-ninh-hinh-su\", \"the-thao\", \"bong-da\", \"bong-da-quoc-te\",\n",
    "         \"lich-thi-dau\", \"golf\", \"tennis\", \"y-te\", \"tlv-canh-doi\",\n",
    "         \"dinh-duong-am-thuc\", \"lam-dep\", \"cac-loai-benh\", \"van-hoa\",\n",
    "         \"giai-tri\", \"thoi-trang\", \"sach-hay\", \"cach-lam-hay-tu-co-so\",\n",
    "         \"vi-loi-ich-doan-vien\", \"the-gioi-so\", \"vu-khi\", \"quan-su\",\n",
    "         \"nha-dep\", \"quy-hoach\", \"thi-truong-xe\", \"lai-xe-an-toan\",\n",
    "         \"tu-van-lao-dong\", \"xuat-khau-lao-dong\", \"chinh-sach-giao-duc\",\n",
    "         \"tuyen-sinh\", \"chuyen-nha-minh\", \"yeu-360\", \"nuoi-con\", \n",
    "         \"tlv-tin-hoat-dong\"]\n",
    "ld_news = [lao_dong_url + i for i in sub_page_ld]\n",
    "\n",
    "tuoi_tre_url = \"https://tuoitre.vn/\"\n",
    "sub_page_tt = [\"thoi-su\", \"phap-luat\", \"kinh-doanh\", \"the-gioi\", \"van-hoa\", \"giao-duc\", \"suc-khoe\",\n",
    "         \"du-lich\", \"the-thao\", \"cong-nghe\", \"giai-tri\", \"xe\", \"nhip-song-tre\", \"nha-dat\", \"gia-that\", \"ban-doc\"]\n",
    "tt_news = [tuoi_tre_url + i for i in sub_page_tt]\n",
    "\n",
    "dantri_url = \"https://dantri.com.vn/\"\n",
    "sub_page_dt = [\"xa-hoi\", \"kinh-doanh\", \"the-gioi\", \"giai-tri\", \"bat-dong-san\", \"the-thao\",\n",
    "               \"viec-lam\", \"suc-khoe\", \"o-to-xe-may\", \"suc-manh-so\", \"giao-duc\", \"an-sinh\",\n",
    "               \"phap-luat\", \"du-lich\", \"doi-song\", \"tinh-yeu-gioi-tinh\", \"khoa-hoc-cong-nghe\"]\n",
    "dt_news = [dantri_url + i for i in sub_page_dt]\n",
    "\n",
    "vnexpress_url = \"https://vnexpress.net/\"\n",
    "sub_page_vne = [\"thoi-su\", \"the-gioi\", \"kinh-doanh\", \"giai-tri\", \"the-thao\", \n",
    "                \"phap-luat\", \"giao-duc\", \"suc-khoe\", \"doi-song\", \"du-lich\", \n",
    "                \"khoa-hoc\", \"so-hoa\", \"oto-xe-may\", \"bat-dong-san\", \"y-kien\"]\n",
    "vne_news = [vnexpress_url + i for i in sub_page_vne]\n",
    "\n",
    "\n",
    "vnnet_url = \"https://vietnamnet.vn/\"\n",
    "sub_page_vnnet = [\"thoi-su\", \"kinh-doanh\", \"giai-tri\", \"the-thao\", \"the-gioi\",\"giao-duc\",\"van-hoa\",\"doi-song\",\"suc-khoe\"\n",
    "                  ,\"thong-tin-truyen-thong\", \"phap-luat\", \"oto-xe-may\", \"bat-dong-san\", \"du-lich\"]\n",
    "vnnet_news = [vnnet_url + i for i in sub_page_vnnet]\n",
    "\n",
    "\n",
    "doisongphapluat_url = \"https://doisongphapluat.com/\"\n",
    "sub_page_dspl = [\"tin-tuc-1\",\"phap-luat-3\",\"kinh-doanh-17\",\"doi-song-2\",\"tin-the-gioi-10\",\"van-hoa-6\",\"media-179\",\"the-thao-4\",\"giao-duc-63\"]\n",
    "dspl_news = [doisongphapluat_url + i for i in sub_page_dspl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8054d2b7-59cf-489b-9656-60eda88a6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_topic = {\"thoi-su\": \"Thời sự\", \"kinh-te\": \"Kinh tế\", \"the-thao\": \"Thể thao\", \"the-gioi\": \"Thế giới\",\n",
    "                \"giao-duc\": \"Giáo dục\", \"phap-luat\": \"Pháp luật\", \"giai-tri\": \"Giải trí\", \"suc-khoe\": \"Sức khỏe\",\n",
    "                \"khoa-hoc-cong-nghe\": \"Khoa học công nghệ\", \"oto-xe-may\": \"Ô tô xe máy\", \"phong-su-kham-pha\": \"Phóng sự khám phá\",\n",
    "                \"song-ket-noi\": \"Sống kết nối\", \"xa-hoi\": \"Xã hội\", \"moi-truong\": \"Môi trường\", \"giao-thong\": \"Giao thông\",\n",
    "                \"dinh-duong-am-thuc\": \"Dinh dưỡng ẩm thực\", \"lam-dep\": \"Làm đẹp\", \"cac-loai-benh\": \"Các loại bệnh\",\n",
    "                \"van-hoa\": \"Văn hóa\", \"thoi-trang\": \"Thời trang\", \"sach-hay\": \"Sách hay\", \"cach-lam-hay-tu-co-so\": \"Cách làm hay từ cơ sở\",\n",
    "                \"vi-loi-ich-doan-vien\": \"Vì lợi ích đoàn viên\", \"the-gioi-so\": \"Thế giới số\", \"vu-khi\": \"Vũ khí\", \"quan-su\": \"Quân sự\",\n",
    "                \"nha-dep\": \"Nhà đẹp\", \"quy-hoach\": \"Quy hoạch\", \"thi-truong-xe\": \"Thị trường xe\", \"lai-xe-an-toan\": \"Lái xe an toàn\",\n",
    "                \"tu-van-lao-dong\": \"Tư vấn lao động\", \"xuat-khau-lao-dong\": \"Xuất khẩu lao động\", \"chinh-sach-giao-duc\": \"Chính sách giáo dục\",\n",
    "                \"tuyen-sinh\": \"Tuyển sinh\", \"chuyen-nha-minh\": \"Chuyện nhà mình\", \"yeu-360\": \"Yêu 360\", \"nuoi-con\": \"Nuôi con\",\n",
    "                \"tlv-canh-doi\": \"TLV cảnh đời\", \"tlv-tin-hoat-dong\": \"TLV tin hoạt động\", \"bong-da\": \"Bóng đá\",\n",
    "                \"bong-da-quoc-te\": \"Bóng đá quốc tế\", \"lich-thi-dau\": \"Lịch thi đấu\", \"golf\": \"Golf\", \"tennis\": \"Tennis\",\n",
    "                \"y-te\": \"Y tế\", \"an-ninh-hinh-su\": \"An ninh hình sự\", \"tu-van-phap-luat\": \"Tư vấn pháp luật\", \"nhip-song-tre\": \"Nhịp sống trẻ\",\n",
    "                \"nha-dat\": \"Nhà đất\", \"gia-that\": \"Giá thất\", \"ban-doc\": \"Bạn đọc\", \"viec-lam\": \"Việc làm\", \"suc-manh-so\": \"Sức mạnh số\",\n",
    "                \"tinh-yeu-gioi-tinh\": \"Tình yêu giới tính\", \"doi-song\": \"Đời sống\", \"bat-dong-san\": \"Bất động sản\", \"an-sinh\": \"An sinh\",\n",
    "                \"kinh-doanh\": \"Kinh doanh\", \"phap-luat\": \"Pháp luật\", \"du-lich\": \"Du lịch\", \"cong-nghe\": \"Công nghệ\", \"xe\": \"Xe\", \"y-kien\": \"Ý kiến\",\n",
    "                \"so-hoa\": \"Số hóa\", \"khoa-hoc\": \"Khoa học\", \"o-to-xe-may\": \"Ô tô xe máy\", \"thi-truong\": \"Thị trường\", \n",
    "                \"thong-tin-truyen-thong\": \"Thông tin truyền thông\", \"tin-the-gioi\":\"Tin thế giới\", \"media\": \"Media\", \"tin-tuc\": \"Tin tức\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87f1fa55-bd04-4a41-b869-f9519171d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data4 = pd.DataFrame(columns=[\"title\", \"abstract\", \"source\", \"link\", \"topic\", \"time\", \"imglink\"])\n",
    "data4 = pd.read_csv(\"crawled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70b0ef34-0ec6-42c6-b716-433af87d4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_articles_at_vtc(max_sl, url, data4):\n",
    "    articles_list = []\n",
    "    vtc_news_pages = []\n",
    "    vtc_news_pages.append(url+\".html\")\n",
    "    for i in range(2, max_sl):\n",
    "        vtc_news_pages.append(f\"{url}/trang-{i}.html\")\n",
    "\n",
    "\n",
    "    def get_vtc_articles(weburl, data):\n",
    "        response = requests.get(weburl,verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Combine both types of article containers in one loop\n",
    "        article_containers = soup.find_all('article', class_=['clearfix distance', 'distance clearfix'])\n",
    "        \n",
    "        for article in article_containers:\n",
    "            title_element = article.select_one('h3.title-1 a')\n",
    "            abstract_element = article.select_one('p')\n",
    "            img_element = article.select_one('figure img')\n",
    "            time_element = article.select_one('footer span.time-update')\n",
    "            \n",
    "            if title_element and abstract_element and img_element and time_element:\n",
    "                title = title_element.get('title', 'No Title')\n",
    "                link = title_element.get('href', '#')\n",
    "                abstract = abstract_element.text.strip()\n",
    "                imglink = img_element.get('data-src', '')\n",
    "                time = time_element.text.strip().split(' ')\n",
    "                topic = url.split('/')[-1]\n",
    "                topic = topic[:topic.rfind(\"-\")]\n",
    "                \n",
    "                if len(time) >= 2:\n",
    "                    time_formatted = f\"{time[1]} {time[0]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "                \n",
    "                # Store each article information in a dictionary and add it to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo VTC\",\n",
    "                    \"link\": f\"https://vtcnews.vn{link}\",\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        \n",
    "        return pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "\n",
    "    for page in vtc_news_pages:\n",
    "        data4 = get_vtc_articles(page, data4)\n",
    "    \n",
    "    return data4\n",
    "\n",
    "\n",
    "def find_all_articles_at_ld(max_sl, url, data4):\n",
    "    ld_new_pages = []\n",
    "    for i in range(1, max_sl):\n",
    "        ld_new_pages.append(f\"{url}?page={i}\")\n",
    "\n",
    "    def get_ld_articles(url):\n",
    "        response = requests.get(url,verify=False)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles_list = []\n",
    "        article_containers = soup.find_all(name = 'article')\n",
    "\n",
    "        for article in article_containers:\n",
    "            title_element = article.select_one('h2',class_=\"title\")\n",
    "            link_title_element = article.select_one(\"a\", class_=\"link-title\")\n",
    "            abstract_element = article.select_one(\".chapeau\")\n",
    "            img_element = article.select_one('figure img')\n",
    "            time_element = article.select_one('span.time')\n",
    "            \n",
    "            if title_element and abstract_element and img_element and time_element:\n",
    "                title = title_element.text\n",
    "                link = link_title_element.get('href')\n",
    "                abstract = abstract_element.text\n",
    "                imglink = img_element.get('data-src')\n",
    "                time = time_element.text.strip().split(\" \")\n",
    "                topic = url.split('/')[-1]\n",
    "                topic = topic[:topic.find(\"?\")]\n",
    "\n",
    "                # Format the time into a readable format\n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[0]} {time[1]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "\n",
    "                # Append the article's information to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo Lao Động\",\n",
    "                    \"link\": link,\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        return articles_list\n",
    "    \n",
    "    for page in ld_new_pages:\n",
    "        data4 = pd.concat([data4, pd.DataFrame(get_ld_articles(page))], ignore_index=True)\n",
    "\n",
    "    return data4\n",
    "\n",
    "\n",
    "\n",
    "def find_all_articles_at_tt(url, data4):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    screen_height = browser.execute_script(\"return window.screen.height;\") * 2 # Browser window height times 3  \n",
    "    i = 1\n",
    "    while True:\n",
    "        browser.execute_script(f\"window.scrollTo(0, {screen_height * i});\")\n",
    "        i += 1\n",
    "        t.sleep(1)\n",
    "        if browser.execute_script(\"return document.body.scrollHeight;\") < screen_height * i:\n",
    "            break\n",
    "    \n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    articles_list = []\n",
    "    # select all (div tag have box-category-item class) inside of a div have id load-list-news\n",
    "    article_containers = soup.select(\"#load-list-news div.box-category-item\")\n",
    "\n",
    "    for article in article_containers:\n",
    "        title_element = article.select_one('h3.box-title-text')\n",
    "        link_title_element = article.select_one(\"a.box-category-link-title\")\n",
    "        abstract_element = article.select_one(\"p.box-category-sapo\")\n",
    "        img_element = article.select_one('img.box-category-avatar')\n",
    "        time_element = None # not found in the page\n",
    "        \n",
    "        if title_element and abstract_element and img_element:\n",
    "            # remove the '\\n' at first of title\n",
    "            title = title_element.text.strip()\n",
    "            link = link_title_element.get('href')\n",
    "            abstract = abstract_element.text\n",
    "            imglink = img_element.get('src')\n",
    "            time = None\n",
    "            topic = url.split('/')[3]\n",
    "            \n",
    "            # Format the time into a readable format\n",
    "            if time and len(time) >= 2:\n",
    "                time_formatted = f\"{time[0]} {time[1]}\"\n",
    "            else:\n",
    "                time_formatted = \"Unknown Time\"\n",
    "\n",
    "            # Append the article's information to the list\n",
    "            articles_list.append({\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"source\": \"Báo Tuổi Trẻ\",\n",
    "                \"link\": f\"https://tuoitre.vn{link}\",\n",
    "                \"topic\": dict_topic[topic],\n",
    "                \"time\": time_formatted,\n",
    "                \"imglink\": imglink\n",
    "            })\n",
    "    \n",
    "    data4 = pd.concat([data4, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "    return data4\n",
    "\n",
    "def find_all_articles_at_dt(maxsl, url, data4):\n",
    "    dt_news_pages = []\n",
    "    dt_news_pages.append(url+\".htm\")\n",
    "    for i in range(2, maxsl):\n",
    "        dt_news_pages.append(f\"{url}/trang-{i}.htm\")\n",
    "\n",
    "\n",
    "    def get_dt_articles(web_url,data):\n",
    "        response = requests.get(web_url,verify=False)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles_list = []\n",
    "        article_containers = soup.find_all(\"article\", class_=\"article-item\")\n",
    "\n",
    "        for article in article_containers:\n",
    "            # tag a have dt-text-black-mine class\n",
    "            title_element = article.select_one('a.dt-text-black-mine')\n",
    "            # inside div tag have class article-excerpt and inside an a tag\n",
    "            abstract_element = article.select_one('div.article-excerpt a')\n",
    "            # img tag have 2 class entered and loaded\n",
    "            img_element = article.select_one('img')\n",
    "            time_element = None\n",
    "            \n",
    "            if title_element and abstract_element and img_element:\n",
    "                title = title_element.text\n",
    "                link = title_element.get('href')\n",
    "                abstract = abstract_element.text\n",
    "                imglink = img_element.get('data-src')\n",
    "                if(imglink == None): # if imglink['data-src'] is None, since src could be a data:base64 and not a link\n",
    "                    continue\n",
    "                time = None\n",
    "                topic = url.split('/')[-1]\n",
    "                \n",
    "                # Format the time into a readable format\n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[0]} {time[1]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "\n",
    "                # Append the article's information to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo Dân Trí\",\n",
    "                    \"link\": f\"https://dantri.com.vn{link}\",\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "    \n",
    "        data = pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "        return data\n",
    "        \n",
    "    for page in dt_news_pages:\n",
    "        data4 = get_dt_articles(page, data4)\n",
    "\n",
    "    return data4\n",
    "\n",
    "def find_all_articles_at_vnexpress(maxsl, url, data4):\n",
    "    vne_news_pages = []\n",
    "    vne_news_pages.append(url)\n",
    "    for i in range(2, maxsl):\n",
    "        vne_news_pages.append(f\"{url}-p{i}\")\n",
    "\n",
    "    def get_vne_articles(web_url, data):\n",
    "        try:\n",
    "            response = requests.get(web_url,verify=False)\n",
    "        except:\n",
    "            return data\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles_list = []\n",
    "        article_containers = soup.find_all(\"article\")\n",
    "\n",
    "        for article in article_containers:\n",
    "            # tag have class title-news\n",
    "            title_element = article.select_one('h3.title-news a')\n",
    "            abstract_element = article.select_one('p.description a')\n",
    "            # if abstract_element have a span tag inside, add that span tag to abstract_element\n",
    "            if abstract_element and abstract_element.select_one('span'):\n",
    "                location_element = abstract_element.select_one('span')\n",
    "            else:\n",
    "                location_element = None\n",
    "            # img inside of article tag\n",
    "            img_element = article.select_one('img')\n",
    "            time_element = None\n",
    "            \n",
    "            if title_element and abstract_element and img_element:\n",
    "                title = title_element.text.strip()\n",
    "                link = title_element.get('href')\n",
    "                abstract = abstract_element.text.strip()\n",
    "                if location_element:\n",
    "                    abstract = location_element.text + \" \" + abstract\n",
    "                imglink = img_element.get('data-src')\n",
    "                if(imglink == None): # if imglink['data-src'] is None, since src could be a data:base64 and not a link\n",
    "                    continue\n",
    "                time = None\n",
    "                topic = url.split('/')[-1]\n",
    "                \n",
    "                # Format the time into a readable format\n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[0]} {time[1]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "\n",
    "                # Append the article's information to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo VnExpress\",\n",
    "                    \"link\": link,\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        \n",
    "        return pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "\n",
    "    for page in vne_news_pages:\n",
    "        data4 = get_vne_articles(page, data4)\n",
    "    \n",
    "    return data4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_all_articles_at_vnnet(maxsl, url, data4):\n",
    "    vnnet_news_pages = []\n",
    "    vnnet_news_pages.append(url)\n",
    "    for i in range(2, maxsl):\n",
    "        vnnet_news_pages.append(f\"{url}-page{i}\")\n",
    "\n",
    "    def get_vnnet_articles(web_url, data):\n",
    "        response = requests.get(web_url,verify=False)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles_list = []\n",
    "        # find all div with class version-news\n",
    "        article_containers = soup.find_all(\"div\", class_=\"version-news\")\n",
    "\n",
    "        for article in article_containers:\n",
    "            # any h tag with first a tag inside\n",
    "            title_element = article.select_one('h3 a')\n",
    "            abstract_element = article.select_one('p')\n",
    "            img_element = article.select_one('img')\n",
    "            \n",
    "            if title_element and abstract_element and img_element:\n",
    "                title = title_element.text.strip()\n",
    "                link = title_element.get('href')\n",
    "                abstract = abstract_element.text.strip()\n",
    "                imglink = img_element.get('data-srcset')\n",
    "                time = None\n",
    "                topic = url.split('/')[-1]\n",
    "                \n",
    "                # Format the time into a readable format\n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[0]} {time[1]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "\n",
    "                # Append the article's information to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo VietnamNet\",\n",
    "                    \"link\": \"https://vietnamnet.vn\" + link,\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        \n",
    "        return pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "\n",
    "    for page in vnnet_news_pages:\n",
    "        data4 = get_vnnet_articles(page, data4)\n",
    "    \n",
    "    return data4\n",
    "\n",
    "\n",
    "def find_all_articles_at_dspl(max_sl, url, data4):\n",
    "    articles_list = []\n",
    "    dspl_news_pages = []\n",
    "    dspl_news_pages.append(url+\".html\")\n",
    "    for i in range(2, max_sl):\n",
    "        dspl_news_pages.append(f\"{url}/trang-{i}.html\")\n",
    "\n",
    "\n",
    "    def get_dspl_articles(weburl, data):\n",
    "        response = requests.get(weburl,verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Combine both types of article containers in one loop\n",
    "        article_containers = soup.find_all('article', class_=['clearfix distance'])\n",
    "\n",
    "        for article in article_containers:\n",
    "            title_element = article.select_one('h3.title-1 a')\n",
    "            abstract_element = article.select_one('p')\n",
    "            img_element = article.select_one('figure img')\n",
    "            \n",
    "            if title_element and abstract_element and img_element:\n",
    "                title = title_element.get('title', 'No Title')\n",
    "                link = title_element.get('href', '#')\n",
    "                abstract = abstract_element.text.strip()\n",
    "                imglink = img_element.get('data-src', '')\n",
    "                time = None\n",
    "                topic = url.split('/')[-1]\n",
    "                topic = topic[:topic.rfind(\"-\")]\n",
    "                \n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[1]} {time[0]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "                \n",
    "                # Store each article information in a dictionary and add it to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo Đời Sống & Pháp Luật\",\n",
    "                    \"link\": f\"https://doisongphapluat.com.vn{link}\",\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        \n",
    "        return pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "\n",
    "    for page in dspl_news_pages:\n",
    "        data4 = get_dspl_articles(page, data4)\n",
    "    \n",
    "    return data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216abb9-c520-45c4-91a7-9930306a0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(vtc_news))):\n",
    "    data4 = find_all_articles_at_vtc(30, vtc_news[i], data4) # 30 pages limit\n",
    "for i in tqdm(range(0, len(ld_news))):\n",
    "    data4 = find_all_articles_at_ld(50, ld_news[i], data4) # not defined the limit\n",
    "for i in tqdm(range(0, len(tt_news))):\n",
    "    data4 = find_all_articles_at_tt(tt_news[i], data4)\n",
    "for i in tqdm(range(0, len(dt_news))):\n",
    "    data4 = find_all_articles_at_dt(30, dt_news[i], data4) # 30 pages limit\n",
    "for i in tqdm(range(0, len(vne_news))):\n",
    "    data4 = find_all_articles_at_vnexpress(20, vne_news[i], data4) # 20 pages limit\n",
    "for i in tqdm(range(0, len((vnnet_news)))):\n",
    "    data4 = find_all_articles_at_vnnet(50, vnnet_news[i], data4) # not sure about the limit\n",
    "for i in tqdm(range(0, len(dspl_news))):\n",
    "    data4 = find_all_articles_at_dspl(30, dspl_news[i], data4) # 30 pages limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a8f25bc-aa37-4e5f-a86d-8f7c033942df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>topic</th>\n",
       "      <th>time</th>\n",
       "      <th>imglink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bình Định yêu cầu minh bạch trong quản lý tiền...</td>\n",
       "      <td>Bình Định - Về thực hiện quản lý tiền công đức...</td>\n",
       "      <td>Báo Lao Động</td>\n",
       "      <td>https://laodong.vn/xa-hoi/binh-dinh-yeu-cau-mi...</td>\n",
       "      <td>Xã hội</td>\n",
       "      <td>11/10/2024 16:23</td>\n",
       "      <td>https://media-cdn-v2.laodong.vn/storage/newspo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xoá mù chữ cho người lớn tuổi ở miền núi Quảng...</td>\n",
       "      <td>Ban chỉ đạo Phổ cập giáo dục, xoá mù chữ huyện...</td>\n",
       "      <td>Báo Lao Động</td>\n",
       "      <td>https://laodong.vn/video/xoa-mu-chu-cho-nguoi-...</td>\n",
       "      <td>Xã hội</td>\n",
       "      <td>11/10/2024 16:02</td>\n",
       "      <td>https://media-cdn-v2.laodong.vn/storage/newspo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Báo Lao Động khánh thành trụ sở văn phòng đại ...</td>\n",
       "      <td>Quảng Ninh - Sáng 11.10, báo Lao Động khánh th...</td>\n",
       "      <td>Báo Lao Động</td>\n",
       "      <td>https://laodong.vn/xa-hoi/bao-lao-dong-khanh-t...</td>\n",
       "      <td>Xã hội</td>\n",
       "      <td>11/10/2024 15:29</td>\n",
       "      <td>https://media-cdn-v2.laodong.vn/storage/newspo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cháy tầng hầm Nhà hát Trưng Vương Đà Nẵng</td>\n",
       "      <td>Trưa 11.10, UBND quận Hải Châu xác nhận xảy ra...</td>\n",
       "      <td>Báo Lao Động</td>\n",
       "      <td>https://laodong.vn/ban-tin/chay-tang-ham-nha-h...</td>\n",
       "      <td>Xã hội</td>\n",
       "      <td>11/10/2024 14:30</td>\n",
       "      <td>https://media-cdn-v2.laodong.vn/storage/newspo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lạ lẫm quán ăn ở TPHCM, khách hàng trả phí 1 n...</td>\n",
       "      <td>TPHCM - Sau khi tâm sự với con gái về nguyện v...</td>\n",
       "      <td>Báo Lao Động</td>\n",
       "      <td>https://laodong.vn/video/la-lam-quan-an-o-tphc...</td>\n",
       "      <td>Xã hội</td>\n",
       "      <td>11/10/2024 14:29</td>\n",
       "      <td>https://media-cdn-v2.laodong.vn/storage/newspo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180288</th>\n",
       "      <td>Những trường điểm chuẩn thấp, chưa đến 5 điểm ...</td>\n",
       "      <td>Bên cạnh nhiều trường lấy điểm chuẩn vượt ngưỡ...</td>\n",
       "      <td>Báo Đời Sống &amp; Pháp Luật</td>\n",
       "      <td>https://doisongphapluat.com.vn/nhung-truong-di...</td>\n",
       "      <td>Giáo dục</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn-i.doisongphapluat.com.vn/resize/me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180289</th>\n",
       "      <td>Vụ hàng trăm phụ huynh  \"vây \" trường học ở Hà...</td>\n",
       "      <td>Liên quan đến vụ hàng trăm phụ huynh \"vây\" trư...</td>\n",
       "      <td>Báo Đời Sống &amp; Pháp Luật</td>\n",
       "      <td>https://doisongphapluat.com.vn/vu-hang-tram-ph...</td>\n",
       "      <td>Giáo dục</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn-i.doisongphapluat.com.vn/resize/me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180290</th>\n",
       "      <td>Vụ lùm xùm thi lớp 10 ở Thái Bình: Đình chỉ Gi...</td>\n",
       "      <td>UBND tỉnh Thái Bình gia hạn tạm đình chỉ công ...</td>\n",
       "      <td>Báo Đời Sống &amp; Pháp Luật</td>\n",
       "      <td>https://doisongphapluat.com.vn/vu-lum-xum-thi-...</td>\n",
       "      <td>Giáo dục</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn-i.doisongphapluat.com.vn/resize/me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180291</th>\n",
       "      <td>Giá phòng trọ cao ngất ngưởng, sinh viên tỉnh ...</td>\n",
       "      <td>Nhiều tân sinh viên cảm thấy choáng ngợp trước...</td>\n",
       "      <td>Báo Đời Sống &amp; Pháp Luật</td>\n",
       "      <td>https://doisongphapluat.com.vn/gia-phong-tro-c...</td>\n",
       "      <td>Giáo dục</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn-i.doisongphapluat.com.vn/resize/me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180292</th>\n",
       "      <td>Tuyển sinh 2024: Tân sinh viên nhập học cần nh...</td>\n",
       "      <td>Sau khi hoàn tất việc xác nhận nhập học trực t...</td>\n",
       "      <td>Báo Đời Sống &amp; Pháp Luật</td>\n",
       "      <td>https://doisongphapluat.com.vn/tuyen-sinh-2024...</td>\n",
       "      <td>Giáo dục</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn-i.doisongphapluat.com.vn/resize/me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70589 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "0       Bình Định yêu cầu minh bạch trong quản lý tiền...   \n",
       "1       Xoá mù chữ cho người lớn tuổi ở miền núi Quảng...   \n",
       "2       Báo Lao Động khánh thành trụ sở văn phòng đại ...   \n",
       "3               Cháy tầng hầm Nhà hát Trưng Vương Đà Nẵng   \n",
       "4       Lạ lẫm quán ăn ở TPHCM, khách hàng trả phí 1 n...   \n",
       "...                                                   ...   \n",
       "180288  Những trường điểm chuẩn thấp, chưa đến 5 điểm ...   \n",
       "180289  Vụ hàng trăm phụ huynh  \"vây \" trường học ở Hà...   \n",
       "180290  Vụ lùm xùm thi lớp 10 ở Thái Bình: Đình chỉ Gi...   \n",
       "180291  Giá phòng trọ cao ngất ngưởng, sinh viên tỉnh ...   \n",
       "180292  Tuyển sinh 2024: Tân sinh viên nhập học cần nh...   \n",
       "\n",
       "                                                 abstract  \\\n",
       "0       Bình Định - Về thực hiện quản lý tiền công đức...   \n",
       "1       Ban chỉ đạo Phổ cập giáo dục, xoá mù chữ huyện...   \n",
       "2       Quảng Ninh - Sáng 11.10, báo Lao Động khánh th...   \n",
       "3       Trưa 11.10, UBND quận Hải Châu xác nhận xảy ra...   \n",
       "4       TPHCM - Sau khi tâm sự với con gái về nguyện v...   \n",
       "...                                                   ...   \n",
       "180288  Bên cạnh nhiều trường lấy điểm chuẩn vượt ngưỡ...   \n",
       "180289  Liên quan đến vụ hàng trăm phụ huynh \"vây\" trư...   \n",
       "180290  UBND tỉnh Thái Bình gia hạn tạm đình chỉ công ...   \n",
       "180291  Nhiều tân sinh viên cảm thấy choáng ngợp trước...   \n",
       "180292  Sau khi hoàn tất việc xác nhận nhập học trực t...   \n",
       "\n",
       "                          source  \\\n",
       "0                   Báo Lao Động   \n",
       "1                   Báo Lao Động   \n",
       "2                   Báo Lao Động   \n",
       "3                   Báo Lao Động   \n",
       "4                   Báo Lao Động   \n",
       "...                          ...   \n",
       "180288  Báo Đời Sống & Pháp Luật   \n",
       "180289  Báo Đời Sống & Pháp Luật   \n",
       "180290  Báo Đời Sống & Pháp Luật   \n",
       "180291  Báo Đời Sống & Pháp Luật   \n",
       "180292  Báo Đời Sống & Pháp Luật   \n",
       "\n",
       "                                                     link     topic  \\\n",
       "0       https://laodong.vn/xa-hoi/binh-dinh-yeu-cau-mi...    Xã hội   \n",
       "1       https://laodong.vn/video/xoa-mu-chu-cho-nguoi-...    Xã hội   \n",
       "2       https://laodong.vn/xa-hoi/bao-lao-dong-khanh-t...    Xã hội   \n",
       "3       https://laodong.vn/ban-tin/chay-tang-ham-nha-h...    Xã hội   \n",
       "4       https://laodong.vn/video/la-lam-quan-an-o-tphc...    Xã hội   \n",
       "...                                                   ...       ...   \n",
       "180288  https://doisongphapluat.com.vn/nhung-truong-di...  Giáo dục   \n",
       "180289  https://doisongphapluat.com.vn/vu-hang-tram-ph...  Giáo dục   \n",
       "180290  https://doisongphapluat.com.vn/vu-lum-xum-thi-...  Giáo dục   \n",
       "180291  https://doisongphapluat.com.vn/gia-phong-tro-c...  Giáo dục   \n",
       "180292  https://doisongphapluat.com.vn/tuyen-sinh-2024...  Giáo dục   \n",
       "\n",
       "                    time                                            imglink  \n",
       "0       11/10/2024 16:23  https://media-cdn-v2.laodong.vn/storage/newspo...  \n",
       "1       11/10/2024 16:02  https://media-cdn-v2.laodong.vn/storage/newspo...  \n",
       "2       11/10/2024 15:29  https://media-cdn-v2.laodong.vn/storage/newspo...  \n",
       "3       11/10/2024 14:30  https://media-cdn-v2.laodong.vn/storage/newspo...  \n",
       "4       11/10/2024 14:29  https://media-cdn-v2.laodong.vn/storage/newspo...  \n",
       "...                  ...                                                ...  \n",
       "180288      Unknown Time  https://cdn-i.doisongphapluat.com.vn/resize/me...  \n",
       "180289      Unknown Time  https://cdn-i.doisongphapluat.com.vn/resize/me...  \n",
       "180290      Unknown Time  https://cdn-i.doisongphapluat.com.vn/resize/me...  \n",
       "180291      Unknown Time  https://cdn-i.doisongphapluat.com.vn/resize/me...  \n",
       "180292      Unknown Time  https://cdn-i.doisongphapluat.com.vn/resize/me...  \n",
       "\n",
       "[70589 rows x 7 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4 = data4.drop_duplicates(subset = 'title', keep = 'last')\n",
    "data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "52486427-11b1-441e-8b19-334b483d321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.to_csv('crawled.csv', index = False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
