{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b83c0933-054d-4d41-b48f-30711fb9e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time as t\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# disable warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "425b2120-18af-441a-9eaf-36b564f6e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtc_url = \"https://vtcnews.vn/\"\n",
    "sub_page_vtc=['thoi-su-28', 'kinh-te-29', 'the-thao-34', 'the-gioi-30',\n",
    "              'giao-duc-31', 'phap-luat-32', 'giai-tri-33', 'suc-khoe-35', \n",
    "              'khoa-hoc-cong-nghe-82', 'oto-xe-may-37', \n",
    "              'phong-su-kham-pha-36', 'song-ket-noi-123']\n",
    "vtc_news = [vtc_url + i for i in sub_page_vtc]\n",
    "\n",
    "lao_dong_url = \"https://laodong.vn/\"\n",
    "sub_page_ld = [\"xa-hoi\", \"moi-truong\", \"giao-thong\", \"kinh-doanh\", \n",
    "         \"thoi-su\", \"the-gioi\", \"phap-luat\", \"tu-van-phap-luat\", \n",
    "         \"an-ninh-hinh-su\", \"the-thao\", \"bong-da\", \"bong-da-quoc-te\",\n",
    "         \"lich-thi-dau\", \"golf\", \"tennis\", \"y-te\", \"tlv-canh-doi\",\n",
    "         \"dinh-duong-am-thuc\", \"lam-dep\", \"cac-loai-benh\", \"van-hoa\",\n",
    "         \"giai-tri\", \"thoi-trang\", \"sach-hay\", \"cach-lam-hay-tu-co-so\",\n",
    "         \"vi-loi-ich-doan-vien\", \"the-gioi-so\", \"vu-khi\", \"quan-su\",\n",
    "         \"nha-dep\", \"quy-hoach\", \"thi-truong-xe\", \"lai-xe-an-toan\",\n",
    "         \"tu-van-lao-dong\", \"xuat-khau-lao-dong\", \"chinh-sach-giao-duc\",\n",
    "         \"tuyen-sinh\", \"chuyen-nha-minh\", \"yeu-360\", \"nuoi-con\", \n",
    "         \"tlv-tin-hoat-dong\"]\n",
    "ld_news = [lao_dong_url + i for i in sub_page_ld]\n",
    "\n",
    "tuoi_tre_url = \"https://tuoitre.vn/\"\n",
    "sub_page_tt = [\"thoi-su\", \"phap-luat\", \"kinh-doanh\", \"the-gioi\", \"van-hoa\", \"giao-duc\", \"suc-khoe\",\n",
    "         \"du-lich\", \"the-thao\", \"cong-nghe\", \"giai-tri\", \"xe\", \"nhip-song-tre\", \"nha-dat\", \"gia-that\", \"ban-doc\"]\n",
    "tt_news = [tuoi_tre_url + i for i in sub_page_tt]\n",
    "\n",
    "dantri_url = \"https://dantri.com.vn/\"\n",
    "sub_page_dt = [\"xa-hoi\", \"kinh-doanh\", \"the-gioi\", \"giai-tri\", \"bat-dong-san\", \"the-thao\",\n",
    "               \"viec-lam\", \"suc-khoe\", \"o-to-xe-may\", \"suc-manh-so\", \"giao-duc\", \"an-sinh\",\n",
    "               \"phap-luat\", \"du-lich\", \"doi-song\", \"tinh-yeu-gioi-tinh\", \"khoa-hoc-cong-nghe\"]\n",
    "dt_news = [dantri_url + i for i in sub_page_dt]\n",
    "\n",
    "vnexpress_url = \"https://vnexpress.net/\"\n",
    "sub_page_vne = [\"thoi-su\", \"the-gioi\", \"kinh-doanh\", \"giai-tri\", \"the-thao\", \n",
    "                \"phap-luat\", \"giao-duc\", \"suc-khoe\", \"doi-song\", \"du-lich\", \n",
    "                \"khoa-hoc\", \"so-hoa\", \"oto-xe-may\", \"bat-dong-san\", \"y-kien\"]\n",
    "vne_news = [vnexpress_url + i for i in sub_page_vne]\n",
    "\n",
    "\n",
    "vnnet_url = \"https://vietnamnet.vn/\"\n",
    "sub_page_vnnet = [\"thoi-su\", \"kinh-doanh\", \"giai-tri\", \"the-thao\", \"the-gioi\",\"giao-duc\",\"van-hoa\",\"doi-song\",\"suc-khoe\"\n",
    "                  ,\"thong-tin-truyen-thong\", \"phap-luat\", \"oto-xe-may\", \"bat-dong-san\", \"du-lich\"]\n",
    "vnnet_news = [vnnet_url + i for i in sub_page_vnnet]\n",
    "\n",
    "\n",
    "doisongphapluat_url = \"https://doisongphapluat.com/\"\n",
    "sub_page_dspl = [\"tin-tuc-1\",\"phap-luat-3\",\"kinh-doanh-17\",\"doi-song-2\",\"tin-the-gioi-10\",\"van-hoa-6\",\"media-179\",\"the-thao-4\",\"giao-duc-63\"]\n",
    "dspl_news = [doisongphapluat_url + i for i in sub_page_dspl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8054d2b7-59cf-489b-9656-60eda88a6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_topic = {\"thoi-su\": \"Thời sự\", \"kinh-te\": \"Kinh tế\", \"the-thao\": \"Thể thao\", \"the-gioi\": \"Thế giới\",\n",
    "                \"giao-duc\": \"Giáo dục\", \"phap-luat\": \"Pháp luật\", \"giai-tri\": \"Giải trí\", \"suc-khoe\": \"Sức khỏe\",\n",
    "                \"khoa-hoc-cong-nghe\": \"Khoa học công nghệ\", \"oto-xe-may\": \"Ô tô xe máy\", \"phong-su-kham-pha\": \"Phóng sự khám phá\",\n",
    "                \"song-ket-noi\": \"Sống kết nối\", \"xa-hoi\": \"Xã hội\", \"moi-truong\": \"Môi trường\", \"giao-thong\": \"Giao thông\",\n",
    "                \"dinh-duong-am-thuc\": \"Dinh dưỡng ẩm thực\", \"lam-dep\": \"Làm đẹp\", \"cac-loai-benh\": \"Các loại bệnh\",\n",
    "                \"van-hoa\": \"Văn hóa\", \"thoi-trang\": \"Thời trang\", \"sach-hay\": \"Sách hay\", \"cach-lam-hay-tu-co-so\": \"Cách làm hay từ cơ sở\",\n",
    "                \"vi-loi-ich-doan-vien\": \"Vì lợi ích đoàn viên\", \"the-gioi-so\": \"Thế giới số\", \"vu-khi\": \"Vũ khí\", \"quan-su\": \"Quân sự\",\n",
    "                \"nha-dep\": \"Nhà đẹp\", \"quy-hoach\": \"Quy hoạch\", \"thi-truong-xe\": \"Thị trường xe\", \"lai-xe-an-toan\": \"Lái xe an toàn\",\n",
    "                \"tu-van-lao-dong\": \"Tư vấn lao động\", \"xuat-khau-lao-dong\": \"Xuất khẩu lao động\", \"chinh-sach-giao-duc\": \"Chính sách giáo dục\",\n",
    "                \"tuyen-sinh\": \"Tuyển sinh\", \"chuyen-nha-minh\": \"Chuyện nhà mình\", \"yeu-360\": \"Yêu 360\", \"nuoi-con\": \"Nuôi con\",\n",
    "                \"tlv-canh-doi\": \"TLV cảnh đời\", \"tlv-tin-hoat-dong\": \"TLV tin hoạt động\", \"bong-da\": \"Bóng đá\",\n",
    "                \"bong-da-quoc-te\": \"Bóng đá quốc tế\", \"lich-thi-dau\": \"Lịch thi đấu\", \"golf\": \"Golf\", \"tennis\": \"Tennis\",\n",
    "                \"y-te\": \"Y tế\", \"an-ninh-hinh-su\": \"An ninh hình sự\", \"tu-van-phap-luat\": \"Tư vấn pháp luật\", \"nhip-song-tre\": \"Nhịp sống trẻ\",\n",
    "                \"nha-dat\": \"Nhà đất\", \"gia-that\": \"Giá thất\", \"ban-doc\": \"Bạn đọc\", \"viec-lam\": \"Việc làm\", \"suc-manh-so\": \"Sức mạnh số\",\n",
    "                \"tinh-yeu-gioi-tinh\": \"Tình yêu giới tính\", \"doi-song\": \"Đời sống\", \"bat-dong-san\": \"Bất động sản\", \"an-sinh\": \"An sinh\",\n",
    "                \"kinh-doanh\": \"Kinh doanh\", \"phap-luat\": \"Pháp luật\", \"du-lich\": \"Du lịch\", \"cong-nghe\": \"Công nghệ\", \"xe\": \"Xe\", \"y-kien\": \"Ý kiến\",\n",
    "                \"so-hoa\": \"Số hóa\", \"khoa-hoc\": \"Khoa học\", \"o-to-xe-may\": \"Ô tô xe máy\", \"thi-truong\": \"Thị trường\", \n",
    "                \"thong-tin-truyen-thong\": \"Thông tin truyền thông\", \"tin-the-gioi\":\"Tin thế giới\", \"media\": \"Media\", \"tin-tuc\": \"Tin tức\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87f1fa55-bd04-4a41-b869-f9519171d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.DataFrame(columns=[\"title\", \"abstract\", \"source\", \"link\", \"topic\", \"time\", \"imglink\"])\n",
    "# data4 = pd.read_csv(\"crawled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70b0ef34-0ec6-42c6-b716-433af87d4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_articles_at_vtc(max_sl, url, data4):\n",
    "    articles_list = []\n",
    "    vtc_news_pages = []\n",
    "    vtc_news_pages.append(url+\".html\")\n",
    "    for i in range(2, max_sl):\n",
    "        vtc_news_pages.append(f\"{url}/trang-{i}.html\")\n",
    "\n",
    "\n",
    "    def get_vtc_articles(weburl, data):\n",
    "        response = requests.get(weburl,verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Combine both types of article containers in one loop\n",
    "        article_containers = soup.find_all('article', class_=['clearfix distance', 'distance clearfix'])\n",
    "        \n",
    "        for article in article_containers:\n",
    "            title_element = article.select_one('h3.title-1 a')\n",
    "            abstract_element = article.select_one('p')\n",
    "            img_element = article.select_one('figure img')\n",
    "            time_element = article.select_one('footer span.time-update')\n",
    "            \n",
    "            if title_element and abstract_element and img_element and time_element:\n",
    "                title = title_element.get('title', 'No Title')\n",
    "                link = title_element.get('href', '#')\n",
    "                abstract = abstract_element.text.strip()\n",
    "                imglink = img_element.get('data-src', '')\n",
    "                time = time_element.text.strip().split(' ')\n",
    "                topic = url.split('/')[-1]\n",
    "                topic = topic[:topic.rfind(\"-\")]\n",
    "                \n",
    "                if len(time) >= 2:\n",
    "                    time_formatted = f\"{time[1]} {time[0]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "                \n",
    "                # Store each article information in a dictionary and add it to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo VTC\",\n",
    "                    \"link\": f\"https://vtcnews.vn{link}\",\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        \n",
    "        return pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "\n",
    "    for page in vtc_news_pages:\n",
    "        data4 = get_vtc_articles(page, data4)\n",
    "    \n",
    "    return data4\n",
    "\n",
    "\n",
    "def find_all_articles_at_ld(max_sl, url, data4):\n",
    "    ld_new_pages = []\n",
    "    ld_new_pages.append(url)\n",
    "    for i in range(2, max_sl):\n",
    "        ld_new_pages.append(f\"{url}?page={i}\")\n",
    "\n",
    "    def get_ld_articles(url):\n",
    "        response = requests.get(url,verify=False)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles_list = []\n",
    "        article_containers = soup.find_all(name = 'article')\n",
    "\n",
    "        for article in article_containers:\n",
    "            title_element = article.select_one('h2',class_=\"title\")\n",
    "            link_title_element = article.select_one(\"a\", class_=\"link-title\")\n",
    "            abstract_element = article.select_one(\".chapeau\")\n",
    "            img_element = article.select_one('figure img')\n",
    "            time_element = article.select_one('span.time')\n",
    "            \n",
    "            if title_element and abstract_element and img_element and time_element:\n",
    "                title = title_element.text\n",
    "                link = link_title_element.get('href')\n",
    "                abstract = abstract_element.text\n",
    "                imglink = img_element.get('data-src')\n",
    "                time = time_element.text.strip().split(\" \")\n",
    "                topic = url.split('/')[-1]\n",
    "                topic = topic[:topic.find(\"?\") if topic.find(\"?\") != -1 else len(topic)]\n",
    "\n",
    "                # Format the time into a readable format\n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[0]} {time[1]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "\n",
    "                # Append the article's information to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo Lao Động\",\n",
    "                    \"link\": link,\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        return articles_list\n",
    "    \n",
    "    for page in ld_new_pages:\n",
    "        data4 = pd.concat([data4, pd.DataFrame(get_ld_articles(page))], ignore_index=True)\n",
    "\n",
    "    return data4\n",
    "\n",
    "\n",
    "\n",
    "def find_all_articles_at_tt(url, data4):\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    screen_height = browser.execute_script(\"return window.screen.height;\") * 2 # Browser window height times 3  \n",
    "    i = 1\n",
    "    while True:\n",
    "        browser.execute_script(f\"window.scrollTo(0, {screen_height * i});\")\n",
    "        i += 1\n",
    "        t.sleep(1)\n",
    "        if browser.execute_script(\"return document.body.scrollHeight;\") < screen_height * i:\n",
    "            break\n",
    "    \n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    articles_list = []\n",
    "    # select all (div tag have box-category-item class) inside of a div have id load-list-news\n",
    "    article_containers = soup.select(\"#load-list-news div.box-category-item\")\n",
    "\n",
    "    for article in article_containers:\n",
    "        title_element = article.select_one('h3.box-title-text')\n",
    "        link_title_element = article.select_one(\"a.box-category-link-title\")\n",
    "        abstract_element = article.select_one(\"p.box-category-sapo\")\n",
    "        img_element = article.select_one('img.box-category-avatar')\n",
    "        time_element = None # not found in the page\n",
    "        \n",
    "        if title_element and abstract_element and img_element:\n",
    "            # remove the '\\n' at first of title\n",
    "            title = title_element.text.strip()\n",
    "            link = link_title_element.get('href')\n",
    "            abstract = abstract_element.text\n",
    "            imglink = img_element.get('src')\n",
    "            time = None\n",
    "            topic = url.split('/')[3]\n",
    "            \n",
    "            # Format the time into a readable format\n",
    "            if time and len(time) >= 2:\n",
    "                time_formatted = f\"{time[0]} {time[1]}\"\n",
    "            else:\n",
    "                time_formatted = \"Unknown Time\"\n",
    "\n",
    "            # Append the article's information to the list\n",
    "            articles_list.append({\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"source\": \"Báo Tuổi Trẻ\",\n",
    "                \"link\": f\"https://tuoitre.vn{link}\",\n",
    "                \"topic\": dict_topic[topic],\n",
    "                \"time\": time_formatted,\n",
    "                \"imglink\": imglink\n",
    "            })\n",
    "    \n",
    "    data4 = pd.concat([data4, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "    return data4\n",
    "\n",
    "def find_all_articles_at_dt(maxsl, url, data4):\n",
    "    dt_news_pages = []\n",
    "    dt_news_pages.append(url+\".htm\")\n",
    "    for i in range(2, maxsl):\n",
    "        dt_news_pages.append(f\"{url}/trang-{i}.htm\")\n",
    "\n",
    "\n",
    "    def get_dt_articles(web_url,data):\n",
    "        response = requests.get(web_url,verify=False)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles_list = []\n",
    "        article_containers = soup.find_all(\"article\", class_=\"article-item\")\n",
    "\n",
    "        for article in article_containers:\n",
    "            # tag a have dt-text-black-mine class\n",
    "            title_element = article.select_one('a.dt-text-black-mine')\n",
    "            # inside div tag have class article-excerpt and inside an a tag\n",
    "            abstract_element = article.select_one('div.article-excerpt a')\n",
    "            # img tag have 2 class entered and loaded\n",
    "            img_element = article.select_one('img')\n",
    "            time_element = None\n",
    "            \n",
    "            if title_element and abstract_element and img_element:\n",
    "                title = title_element.text\n",
    "                link = title_element.get('href')\n",
    "                abstract = abstract_element.text\n",
    "                imglink = img_element.get('data-src')\n",
    "                if(imglink == None): # if imglink['data-src'] is None, since src could be a data:base64 and not a link\n",
    "                    continue\n",
    "                time = None\n",
    "                topic = url.split('/')[-1]\n",
    "                \n",
    "                # Format the time into a readable format\n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[0]} {time[1]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "\n",
    "                # Append the article's information to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo Dân Trí\",\n",
    "                    \"link\": f\"https://dantri.com.vn{link}\",\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "    \n",
    "        data = pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "        return data\n",
    "        \n",
    "    for page in dt_news_pages:\n",
    "        data4 = get_dt_articles(page, data4)\n",
    "\n",
    "    return data4\n",
    "\n",
    "def find_all_articles_at_vnexpress(maxsl, url, data4):\n",
    "    vne_news_pages = []\n",
    "    vne_news_pages.append(url)\n",
    "    for i in range(2, maxsl):\n",
    "        vne_news_pages.append(f\"{url}-p{i}\")\n",
    "\n",
    "    def get_vne_articles(web_url, data):\n",
    "        try:\n",
    "            response = requests.get(web_url,verify=False)\n",
    "        except:\n",
    "            return data\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles_list = []\n",
    "        article_containers = soup.find_all(\"article\")\n",
    "\n",
    "        for article in article_containers:\n",
    "            # tag have class title-news\n",
    "            title_element = article.select_one('h3.title-news a')\n",
    "            abstract_element = article.select_one('p.description a')\n",
    "            # if abstract_element have a span tag inside, add that span tag to abstract_element\n",
    "            if abstract_element and abstract_element.select_one('span'):\n",
    "                location_element = abstract_element.select_one('span')\n",
    "            else:\n",
    "                location_element = None\n",
    "            # img inside of article tag\n",
    "            img_element = article.select_one('img')\n",
    "            time_element = None\n",
    "            \n",
    "            if title_element and abstract_element and img_element:\n",
    "                title = title_element.text.strip()\n",
    "                link = title_element.get('href')\n",
    "                abstract = abstract_element.text.strip()\n",
    "                if location_element:\n",
    "                    abstract = location_element.text + \" \" + abstract\n",
    "                imglink = img_element.get('data-src')\n",
    "                if(imglink == None): # if imglink['data-src'] is None, since src could be a data:base64 and not a link\n",
    "                    continue\n",
    "                time = None\n",
    "                topic = url.split('/')[-1]\n",
    "                \n",
    "                # Format the time into a readable format\n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[0]} {time[1]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "\n",
    "                # Append the article's information to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo VnExpress\",\n",
    "                    \"link\": link,\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        \n",
    "        return pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "\n",
    "    for page in vne_news_pages:\n",
    "        data4 = get_vne_articles(page, data4)\n",
    "    \n",
    "    return data4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_all_articles_at_vnnet(maxsl, url, data4):\n",
    "    vnnet_news_pages = []\n",
    "    vnnet_news_pages.append(url)\n",
    "    for i in range(2, maxsl):\n",
    "        vnnet_news_pages.append(f\"{url}-page{i}\")\n",
    "\n",
    "    def get_vnnet_articles(web_url, data):\n",
    "        response = requests.get(web_url,verify=False)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        articles_list = []\n",
    "        # find all div with class version-news\n",
    "        article_containers = soup.find_all(\"div\", class_=\"version-news\")\n",
    "\n",
    "        for article in article_containers:\n",
    "            # any h tag with first a tag inside\n",
    "            title_element = article.select_one('h3 a')\n",
    "            abstract_element = article.select_one('p')\n",
    "            img_element = article.select_one('img')\n",
    "            \n",
    "            if title_element and abstract_element and img_element:\n",
    "                title = title_element.text.strip()\n",
    "                link = title_element.get('href')\n",
    "                abstract = abstract_element.text.strip()\n",
    "                imglink = img_element.get('data-srcset')\n",
    "                time = None\n",
    "                topic = url.split('/')[-1]\n",
    "                \n",
    "                # Format the time into a readable format\n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[0]} {time[1]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "\n",
    "                # Append the article's information to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo VietnamNet\",\n",
    "                    \"link\": \"https://vietnamnet.vn\" + link,\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        \n",
    "        return pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "\n",
    "    for page in vnnet_news_pages:\n",
    "        data4 = get_vnnet_articles(page, data4)\n",
    "    \n",
    "    return data4\n",
    "\n",
    "\n",
    "def find_all_articles_at_dspl(max_sl, url, data4):\n",
    "    articles_list = []\n",
    "    dspl_news_pages = []\n",
    "    dspl_news_pages.append(url+\".html\")\n",
    "    for i in range(2, max_sl):\n",
    "        dspl_news_pages.append(f\"{url}/trang-{i}.html\")\n",
    "\n",
    "\n",
    "    def get_dspl_articles(weburl, data):\n",
    "        response = requests.get(weburl,verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Combine both types of article containers in one loop\n",
    "        article_containers = soup.find_all('article', class_=['clearfix distance'])\n",
    "\n",
    "        for article in article_containers:\n",
    "            title_element = article.select_one('h3.title-1 a')\n",
    "            abstract_element = article.select_one('p')\n",
    "            img_element = article.select_one('figure img')\n",
    "            \n",
    "            if title_element and abstract_element and img_element:\n",
    "                title = title_element.get('title', 'No Title')\n",
    "                link = title_element.get('href', '#')\n",
    "                abstract = abstract_element.text.strip()\n",
    "                imglink = img_element.get('data-src', '')\n",
    "                time = None\n",
    "                topic = url.split('/')[-1]\n",
    "                topic = topic[:topic.rfind(\"-\")]\n",
    "                \n",
    "                if time and len(time) >= 2:\n",
    "                    time_formatted = f\"{time[1]} {time[0]}\"\n",
    "                else:\n",
    "                    time_formatted = \"Unknown Time\"\n",
    "                \n",
    "                # Store each article information in a dictionary and add it to the list\n",
    "                articles_list.append({\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"source\": \"Báo Đời Sống & Pháp Luật\",\n",
    "                    \"link\": f\"https://doisongphapluat.com.vn{link}\",\n",
    "                    \"topic\": dict_topic[topic],\n",
    "                    \"time\": time_formatted,\n",
    "                    \"imglink\": imglink\n",
    "                })\n",
    "        \n",
    "        return pd.concat([data, pd.DataFrame(articles_list)], ignore_index=True)\n",
    "\n",
    "    for page in dspl_news_pages:\n",
    "        data4 = get_dspl_articles(page, data4)\n",
    "    \n",
    "    return data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f216abb9-c520-45c4-91a7-9930306a0834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [04:06<00:00, 20.56s/it]\n",
      "100%|██████████| 41/41 [13:45<00:00, 20.14s/it]\n",
      "100%|██████████| 17/17 [08:50<00:00, 31.23s/it]\n",
      "100%|██████████| 15/15 [04:40<00:00, 18.69s/it]\n",
      "100%|██████████| 14/14 [12:02<00:00, 51.61s/it]\n",
      "100%|██████████| 9/9 [06:34<00:00, 43.83s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(vtc_news))):\n",
    "    data4 = find_all_articles_at_vtc(30, vtc_news[i], data4) # 30 pages limit\n",
    "for i in tqdm(range(0, len(ld_news))):\n",
    "    data4 = find_all_articles_at_ld(50, ld_news[i], data4) # not defined the limit\n",
    "for i in tqdm(range(0, len(tt_news))):\n",
    "    data4 = find_all_articles_at_tt(tt_news[i], data4)\n",
    "for i in tqdm(range(0, len(dt_news))):\n",
    "    data4 = find_all_articles_at_dt(30, dt_news[i], data4) # 30 pages limit\n",
    "for i in tqdm(range(0, len(vne_news))):\n",
    "    data4 = find_all_articles_at_vnexpress(20, vne_news[i], data4) # 20 pages limit\n",
    "for i in tqdm(range(0, len((vnnet_news)))):\n",
    "    data4 = find_all_articles_at_vnnet(50, vnnet_news[i], data4) # not sure about the limit\n",
    "for i in tqdm(range(0, len(dspl_news))):\n",
    "    data4 = find_all_articles_at_dspl(30, dspl_news[i], data4) # 30 pages limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a8f25bc-aa37-4e5f-a86d-8f7c033942df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>topic</th>\n",
       "      <th>time</th>\n",
       "      <th>imglink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sửa đổi Luật Điện lực: Yêu cầu xuất phát từ th...</td>\n",
       "      <td>Sau gần 20 năm triển khai thi hành và sửa đổi,...</td>\n",
       "      <td>Báo VTC</td>\n",
       "      <td>https://vtcnews.vn/sua-doi-luat-dien-luc-yeu-c...</td>\n",
       "      <td>Thời sự</td>\n",
       "      <td>22/10/2024 08:52</td>\n",
       "      <td>https://cdn-i.vtcnews.vn/resize/me/upload/2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phát huy tối đa sức mạnh Nhân dân để đất nước ...</td>\n",
       "      <td>Theo ông Đỗ Văn Chiến, để đất nước bước vào kỷ...</td>\n",
       "      <td>Báo VTC</td>\n",
       "      <td>https://vtcnews.vn/phat-huy-toi-da-suc-manh-nh...</td>\n",
       "      <td>Thời sự</td>\n",
       "      <td>22/10/2024 07:00</td>\n",
       "      <td>https://cdn-i.vtcnews.vn/resize/me/upload/2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xem nghệ nhân tỉ mỉ trùng tu cung điện nơi 13 ...</td>\n",
       "      <td>Các nghệ nhân tỉ mỉ vẽ từng họa tiết nhỏ nhất,...</td>\n",
       "      <td>Báo VTC</td>\n",
       "      <td>https://vtcnews.vn/xem-nghe-nhan-ti-mi-trung-t...</td>\n",
       "      <td>Thời sự</td>\n",
       "      <td>22/10/2024 06:39</td>\n",
       "      <td>https://cdn-i.vtcnews.vn/resize/me/upload/2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tin tai nạn giao thông mới nhất ngày 22/10: Ô ...</td>\n",
       "      <td>Ô tô lao xuống vực khiến một người tử vong, tà...</td>\n",
       "      <td>Báo VTC</td>\n",
       "      <td>https://vtcnews.vn/tin-tai-nan-giao-thong-moi-...</td>\n",
       "      <td>Thời sự</td>\n",
       "      <td>22/10/2024 06:27</td>\n",
       "      <td>https://cdn-i.vtcnews.vn/resize/me/upload/2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phố cà phê đường tàu càng cấm càng đông, cấm c...</td>\n",
       "      <td>Phớt lờ lệnh cấm, các quán cà phê đường tàu ở ...</td>\n",
       "      <td>Báo VTC</td>\n",
       "      <td>https://vtcnews.vn/pho-ca-phe-duong-tau-cang-c...</td>\n",
       "      <td>Thời sự</td>\n",
       "      <td>22/10/2024 06:15</td>\n",
       "      <td>https://cdn-i.vtcnews.vn/resize/me/upload/2024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68522</th>\n",
       "      <td>Cá voi lưng gù ăn thịt cả hải cẩu?</td>\n",
       "      <td>Thật may mắn cho loài hải cẩu khi chúng không ...</td>\n",
       "      <td>Báo Tuổi Trẻ</td>\n",
       "      <td>https://tuoitre.vn/ca-voi-lung-gu-an-thit-ca-h...</td>\n",
       "      <td>Giá thất</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn.tuoitre.vn/zoom/260_163/4715847528...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68523</th>\n",
       "      <td>Ngại kết hôn và sinh con, dân Hàn chi bạo tiền...</td>\n",
       "      <td>Trong bối cảnh người trẻ ngại kết hôn và tỉ lệ...</td>\n",
       "      <td>Báo Tuổi Trẻ</td>\n",
       "      <td>https://tuoitre.vn/ngai-ket-hon-va-sinh-con-da...</td>\n",
       "      <td>Giá thất</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn.tuoitre.vn/zoom/260_163/4715847528...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68524</th>\n",
       "      <td>Moo Deng: Con hà mã lùn ở Thái Lan làm gì mà c...</td>\n",
       "      <td>'Hiện tượng mạng' mới nổi tại Thái Lan, chú hà...</td>\n",
       "      <td>Báo Tuổi Trẻ</td>\n",
       "      <td>https://tuoitre.vn/moo-deng-con-ha-ma-lun-o-th...</td>\n",
       "      <td>Giá thất</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn.tuoitre.vn/zoom/260_163/4715847528...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68525</th>\n",
       "      <td>V BTS, Jang Won Young và nhiều sao Hàn khởi ki...</td>\n",
       "      <td>Trong bối cảnh tin giả phát tán tràn lan gây ả...</td>\n",
       "      <td>Báo Tuổi Trẻ</td>\n",
       "      <td>https://tuoitre.vn/v-bts-jang-won-young-va-nhi...</td>\n",
       "      <td>Giá thất</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn.tuoitre.vn/zoom/260_163/4715847528...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68526</th>\n",
       "      <td>Hộp trang sức 'toàn vàng' tìm thấy sau lũ ở Yê...</td>\n",
       "      <td>Trong quá trình giúp người dân thành phố Yên B...</td>\n",
       "      <td>Báo Tuổi Trẻ</td>\n",
       "      <td>https://tuoitre.vn/hop-trang-suc-toan-vang-tim...</td>\n",
       "      <td>Giá thất</td>\n",
       "      <td>Unknown Time</td>\n",
       "      <td>https://cdn.tuoitre.vn/zoom/260_163/4715847528...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68483 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      Sửa đổi Luật Điện lực: Yêu cầu xuất phát từ th...   \n",
       "1      Phát huy tối đa sức mạnh Nhân dân để đất nước ...   \n",
       "2      Xem nghệ nhân tỉ mỉ trùng tu cung điện nơi 13 ...   \n",
       "3      Tin tai nạn giao thông mới nhất ngày 22/10: Ô ...   \n",
       "4      Phố cà phê đường tàu càng cấm càng đông, cấm c...   \n",
       "...                                                  ...   \n",
       "68522                 Cá voi lưng gù ăn thịt cả hải cẩu?   \n",
       "68523  Ngại kết hôn và sinh con, dân Hàn chi bạo tiền...   \n",
       "68524  Moo Deng: Con hà mã lùn ở Thái Lan làm gì mà c...   \n",
       "68525  V BTS, Jang Won Young và nhiều sao Hàn khởi ki...   \n",
       "68526  Hộp trang sức 'toàn vàng' tìm thấy sau lũ ở Yê...   \n",
       "\n",
       "                                                abstract        source  \\\n",
       "0      Sau gần 20 năm triển khai thi hành và sửa đổi,...       Báo VTC   \n",
       "1      Theo ông Đỗ Văn Chiến, để đất nước bước vào kỷ...       Báo VTC   \n",
       "2      Các nghệ nhân tỉ mỉ vẽ từng họa tiết nhỏ nhất,...       Báo VTC   \n",
       "3      Ô tô lao xuống vực khiến một người tử vong, tà...       Báo VTC   \n",
       "4      Phớt lờ lệnh cấm, các quán cà phê đường tàu ở ...       Báo VTC   \n",
       "...                                                  ...           ...   \n",
       "68522  Thật may mắn cho loài hải cẩu khi chúng không ...  Báo Tuổi Trẻ   \n",
       "68523  Trong bối cảnh người trẻ ngại kết hôn và tỉ lệ...  Báo Tuổi Trẻ   \n",
       "68524  'Hiện tượng mạng' mới nổi tại Thái Lan, chú hà...  Báo Tuổi Trẻ   \n",
       "68525  Trong bối cảnh tin giả phát tán tràn lan gây ả...  Báo Tuổi Trẻ   \n",
       "68526  Trong quá trình giúp người dân thành phố Yên B...  Báo Tuổi Trẻ   \n",
       "\n",
       "                                                    link     topic  \\\n",
       "0      https://vtcnews.vn/sua-doi-luat-dien-luc-yeu-c...   Thời sự   \n",
       "1      https://vtcnews.vn/phat-huy-toi-da-suc-manh-nh...   Thời sự   \n",
       "2      https://vtcnews.vn/xem-nghe-nhan-ti-mi-trung-t...   Thời sự   \n",
       "3      https://vtcnews.vn/tin-tai-nan-giao-thong-moi-...   Thời sự   \n",
       "4      https://vtcnews.vn/pho-ca-phe-duong-tau-cang-c...   Thời sự   \n",
       "...                                                  ...       ...   \n",
       "68522  https://tuoitre.vn/ca-voi-lung-gu-an-thit-ca-h...  Giá thất   \n",
       "68523  https://tuoitre.vn/ngai-ket-hon-va-sinh-con-da...  Giá thất   \n",
       "68524  https://tuoitre.vn/moo-deng-con-ha-ma-lun-o-th...  Giá thất   \n",
       "68525  https://tuoitre.vn/v-bts-jang-won-young-va-nhi...  Giá thất   \n",
       "68526  https://tuoitre.vn/hop-trang-suc-toan-vang-tim...  Giá thất   \n",
       "\n",
       "                   time                                            imglink  \n",
       "0      22/10/2024 08:52  https://cdn-i.vtcnews.vn/resize/me/upload/2024...  \n",
       "1      22/10/2024 07:00  https://cdn-i.vtcnews.vn/resize/me/upload/2024...  \n",
       "2      22/10/2024 06:39  https://cdn-i.vtcnews.vn/resize/me/upload/2024...  \n",
       "3      22/10/2024 06:27  https://cdn-i.vtcnews.vn/resize/me/upload/2024...  \n",
       "4      22/10/2024 06:15  https://cdn-i.vtcnews.vn/resize/me/upload/2024...  \n",
       "...                 ...                                                ...  \n",
       "68522      Unknown Time  https://cdn.tuoitre.vn/zoom/260_163/4715847528...  \n",
       "68523      Unknown Time  https://cdn.tuoitre.vn/zoom/260_163/4715847528...  \n",
       "68524      Unknown Time  https://cdn.tuoitre.vn/zoom/260_163/4715847528...  \n",
       "68525      Unknown Time  https://cdn.tuoitre.vn/zoom/260_163/4715847528...  \n",
       "68526      Unknown Time  https://cdn.tuoitre.vn/zoom/260_163/4715847528...  \n",
       "\n",
       "[68483 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data4 = data4.drop_duplicates(subset = 'title', keep = 'last')\n",
    "data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52486427-11b1-441e-8b19-334b483d321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.to_csv('crawled.csv', index = False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
